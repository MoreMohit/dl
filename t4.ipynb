{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dd789b-24d1-455c-81bd-6409a368703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1Ô∏è‚É£ ‚Äî Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Step 2Ô∏è‚É£ ‚Äî Load and Explore Dataset\n",
    "# -------------------------------------------------------------\n",
    "data = pd.read_csv(\"creditcard.csv\")\n",
    "print(\"Dataset shape:\", data.shape)\n",
    "print(data.head())\n",
    "\n",
    "# The dataset contains credit card transactions:\n",
    "# 'Class' column ‚Üí 0 for normal, 1 for fraudulent transactions\n",
    "\n",
    "# Separate features (X) and target label (y)\n",
    "X = data.drop(\"Class\", axis=1)\n",
    "y = data[\"Class\"]\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Step 3Ô∏è‚É£ ‚Äî Preprocess Data (Standardization)\n",
    "# -------------------------------------------------------------\n",
    "# Scale feature values to mean=0 and variance=1\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split dataset into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use only NORMAL transactions for training the Autoencoder\n",
    "X_train_norm = X_train[y_train == 0]\n",
    "print(\"Normal transactions for training:\", X_train_norm.shape[0])\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Step 4Ô∏è‚É£ ‚Äî Build Autoencoder Model\n",
    "# -------------------------------------------------------------\n",
    "input_dim = X_train_norm.shape[1]   # number of features\n",
    "encoding_dim = 14                   # compressed latent space dimension\n",
    "\n",
    "# Encoder: compress input data\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoder = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "encoder = Dense(7, activation='relu')(encoder)    # Bottleneck layer\n",
    "\n",
    "# Decoder: reconstruct the original input\n",
    "decoder = Dense(encoding_dim, activation='relu')(encoder)\n",
    "decoder = Dense(input_dim, activation='sigmoid')(decoder)\n",
    "\n",
    "# Combine encoder + decoder into autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Step 5Ô∏è‚É£ ‚Äî Compile Autoencoder\n",
    "# -------------------------------------------------------------\n",
    "autoencoder.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='mse',          # Mean Squared Error (for reconstruction)\n",
    "    metrics=['mae']      # Mean Absolute Error\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Step 6Ô∏è‚É£ ‚Äî Train Autoencoder (on only normal transactions)\n",
    "# -------------------------------------------------------------\n",
    "history = autoencoder.fit(\n",
    "    X_train_norm, X_train_norm,\n",
    "    epochs=2,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    shuffle=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Step 7Ô∏è‚É£ ‚Äî Evaluate Reconstruction Error\n",
    "# -------------------------------------------------------------\n",
    "# Predict reconstruction of test set\n",
    "reconstructions = autoencoder.predict(X_test)\n",
    "\n",
    "# Compute reconstruction error (MSE per sample)\n",
    "mse = np.mean(np.power(X_test - reconstructions, 2), axis=1)\n",
    "\n",
    "# Set threshold ‚Äî samples with higher error are anomalies\n",
    "threshold = np.percentile(mse, 95)\n",
    "print(\"Reconstruction Error Threshold:\", threshold)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Step 8Ô∏è‚É£ ‚Äî Detect Anomalies\n",
    "# -------------------------------------------------------------\n",
    "y_pred = (mse > threshold).astype(int)\n",
    "\n",
    "# Confusion Matrix and Classification Report\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "# ROC AUC Score (Model Evaluation Metric)\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test, mse))\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Step 9Ô∏è‚É£ ‚Äî Plot Training Loss\n",
    "# -------------------------------------------------------------\n",
    "plt.plot(history.history['loss'], label=\"Train Loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"Validation Loss\")\n",
    "plt.title(\"Autoencoder Training Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Step üîü ‚Äî Count Normal and Fraudulent Transactions\n",
    "# -------------------------------------------------------------\n",
    "print(\"\\nNormal Transactions:\", sum(y == 0))\n",
    "print(\"Fraudulent Transactions:\", sum(y == 1))\n",
    "\n",
    "\n",
    "# | Question                               | Answer                                                                                            |\n",
    "# | -------------------------------------- | ------------------------------------------------------------------------------------------------- |\n",
    "# | What is an Autoencoder?                | A neural network that learns to reconstruct its input ‚Äî used for unsupervised learning.           |\n",
    "# | What is Anomaly Detection?             | Detecting data points that are significantly different from normal patterns.                      |\n",
    "# | Why use Autoencoders for this?         | Because they learn only normal patterns ‚Äî anomalies produce high reconstruction errors.           |\n",
    "# | Why use only normal data for training? | To teach the autoencoder how normal data looks; anomalies should then appear unusual.             |\n",
    "# | What is the Bottleneck layer?          | The smallest hidden layer that holds compressed latent representation of the input.               |\n",
    "# | What is the threshold used for?        | To classify points with reconstruction error above it as anomalies (frauds).                      |\n",
    "# | Why percentile 95?                     | Because top 5% of samples usually represent outliers/anomalies.                                   |\n",
    "# | What is the optimizer?                 | Adam ‚Äî efficient gradient-based optimizer.                                                        |\n",
    "# | What is the loss function used?        | Mean Squared Error (MSE) ‚Äî measures reconstruction difference.                                    |\n",
    "# | What is ROC-AUC Score?                 | Metric to evaluate the model‚Äôs ability to distinguish between normal and fraudulent transactions. |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
