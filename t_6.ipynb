{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db10a9d8-090e-44ba-a3d1-5edcce21a57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 — Import Required Libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds          # for loading tf_flowers dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bec993-1c97-4d90-bbbe-b59c21f2bf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Step 2 — Load and Preprocess Dataset (tf_flowers)\n",
    "# \n",
    "\n",
    "# Load the TensorFlow Flowers dataset (70% for training, 30% for testing)\n",
    "(train_ds, test_ds) = tfds.load(\n",
    "    \"tf_flowers\",\n",
    "    split=[\"train[:70%]\", \"train[:30%]\"],\n",
    "    as_supervised=True  # returns (image, label)\n",
    ")\n",
    "\n",
    "# Define constants\n",
    "IMG_SIZE = (150, 150)   # Resize all images to 150x150 pixels\n",
    "NUM_CLASSES = 5         # There are 5 flower categories in the dataset\n",
    "\n",
    "# Function to preprocess images and labels\n",
    "def preprocess(img, label):\n",
    "    img = tf.image.resize(img, IMG_SIZE)           # Resize all images\n",
    "    img = preprocess_input(img)                    # Normalize images (for VGG16)\n",
    "    label = tf.one_hot(label, NUM_CLASSES)         # Convert labels to one-hot encoding\n",
    "    return img, label\n",
    "\n",
    "# Apply preprocessing, batching, and prefetching for faster loading\n",
    "train_ds = train_ds.map(preprocess).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.map(preprocess).batch(32).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b79260-4742-4ed7-99b9-d718d53ee96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Step 3 — Load Pre-trained Model (VGG16)\n",
    "# \n",
    "\n",
    "# Load VGG16 model trained on ImageNet, without the top (fully connected) layers\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
    "\n",
    "# Freeze base model layers (to keep pre-trained features fixed)\n",
    "base_model.trainable = False\n",
    "\n",
    "# Build a new model by adding custom classification layers on top\n",
    "model = models.Sequential([\n",
    "    base_model,                              # Pre-trained convolutional base\n",
    "    layers.Flatten(),                        # Flatten output for dense layers\n",
    "    layers.Dense(64, activation='relu'),     # Custom dense layer\n",
    "    layers.Dense(NUM_CLASSES, activation='softmax')  # Output layer (5 flower classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62adcd04-9126-4ac9-8c89-d9deee7030b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Step 4 — Compile and Train Model (Initial Training)\n",
    "# \n",
    "\n",
    "# Compile model with optimizer, loss, and evaluation metrics\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train model for 3 epochs (frozen base)\n",
    "history = model.fit(train_ds, validation_data=test_ds, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6955560-77bf-47b6-8cd5-c9f132d10f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Step 5 — Fine-Tune Model (Unfreeze Top Layers)\n",
    "# \n",
    "\n",
    "# Unfreeze last 4 layers of VGG16 for fine-tuning\n",
    "for layer in base_model.layers[-4:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile with a lower learning rate to prevent large updates\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train again for 2 epochs with fine-tuning enabled\n",
    "fine_history = model.fit(train_ds, validation_data=test_ds, epochs=2)\n",
    "\n",
    "# \n",
    "# Step 6 — Evaluate the Final Model\n",
    "# \n",
    "\n",
    "loss, acc = model.evaluate(test_ds)\n",
    "print(\"Final Accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8262fc-8151-49b4-a882-3be93d55cff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Step 7 — Plot Accuracy Graph\n",
    "# \n",
    "\n",
    "# Combine both training phases for visualization\n",
    "plt.plot(history.history['accuracy'] + fine_history.history['accuracy'])\n",
    "plt.title('Training Accuracy (with Fine-tuning)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a99d62-9d30-4575-b2c7-914953a3324e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| Question                             | Answer                                                                                             |\n",
    "# | ------------------------------------ | -------------------------------------------------------------------------------------------------- |\n",
    "# | What is transfer learning?           | Using a pre-trained model (like VGG16) trained on a large dataset to solve a new, smaller problem. |\n",
    "# | Why use VGG16?                       | It is a well-known CNN architecture trained on ImageNet; great for feature extraction.             |\n",
    "# | What does “include_top=False” mean?  | It removes the original classification layers so we can add our own.                               |\n",
    "# | Why freeze the convolutional layers? | To retain previously learned features and prevent retraining from scratch.                         |\n",
    "# | What is fine-tuning?                 | Unfreezing a few top layers of a pre-trained model to improve performance on new data.             |\n",
    "# | What optimizer is used?              | Adam optimizer (adaptive learning rate).                                                           |\n",
    "# | What is the loss function?           | Categorical crossentropy, suitable for multi-class classification.                                 |\n",
    "# | How many output classes are there?   | 5 (since the dataset `tf_flowers` has 5 flower types).                                             |\n",
    "# | What is the image size used?         | All images resized to 150×150 pixels.                                                              |\n",
    "# | What is one-hot encoding?            | It converts categorical labels (0–4) into binary vectors like [1,0,0,0,0].                         |\n",
    "\n",
    "# #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca365ed5-c1b4-4897-94fc-43bb37799a98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
